{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c15dd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from random import uniform, seed\n",
    "import numpy as np \n",
    "import time\n",
    "from operator import itemgetter # for sort list of list\n",
    "from math import e\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "from skimage.io import imread\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, concatenate\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "\n",
    "from keras.models import Model\n",
    "import networkx as nx\n",
    " \n",
    "\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from keras.models import load_model\n",
    "from keras import models\n",
    "\n",
    "from decimal import Decimal\n",
    "import tensorflow as tf    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cef19",
   "metadata": {},
   "source": [
    "# Metrics based on node degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2aa9fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_D_1_weights_all_nodes(G):    \n",
    "   dic_D_1_Nodes={}\n",
    "   for u in G: \n",
    "       dic_D_1_Nodes[u]=nx.degree(G,u) \n",
    "   return dic_D_1_Nodes\n",
    "\n",
    "\n",
    "def dic_D_2_weights_all_nodes(G,dic_D_1_Nodes):    \n",
    "    \n",
    "   dic_D_2_Nodes={}\n",
    "   for u in G:\n",
    "       Tv=[n for n in G.neighbors(u)] # neighbors of v\n",
    "       D2=dic_D_1_Nodes[(u)]\n",
    "       for v in Tv:\n",
    "           D2+=dic_D_1_Nodes[(v)]\n",
    "       dic_D_2_Nodes[u]=D2        \n",
    "   return dic_D_2_Nodes\n",
    "\n",
    "\n",
    "def dic_D_3_weights_all_nodes(G,dic_D_2_Nodes):        \n",
    "   dic_D_3_Nodes={}\n",
    "   for u in G:\n",
    "       Tv=[n for n in G.neighbors(u)] # neighbors of v\n",
    "       D3=dic_D_2_Nodes[(u)]\n",
    "       for v in Tv:\n",
    "           D3+=dic_D_2_Nodes[(v)]\n",
    "       dic_D_3_Nodes[u]=D3       \n",
    "   return dic_D_3_Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634322b0",
   "metadata": {},
   "source": [
    "# Metrics based on H-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63ed7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def H_index(G,node):\n",
    "   Tv=[n for n in G.neighbors(node)] # neighbors of v.\n",
    "   # sorting in ascending order\n",
    "   citations=[nx.degree(G,v) for v in Tv ]\n",
    "   citations.sort()\n",
    "     \n",
    "   # iterating over the list\n",
    "   for i, cited in enumerate(citations):\n",
    "         \n",
    "       # finding current result\n",
    "       result = len(citations) - i          \n",
    "       # if result is less than or equal\n",
    "       # to cited then return result\n",
    "       if result <= cited:\n",
    "           return result           \n",
    "   return 0\n",
    "\n",
    "def H_index_weights_of_All_nodes(G):\n",
    "    h_index_1_Nodes={} \n",
    "    h_index_2_Nodes={}\n",
    "    h_index_3_Nodes={} \n",
    "    \n",
    "    for u in G:\n",
    "        H=H_index(G,u)\n",
    "        h_index_1_Nodes[u]=H \n",
    "\n",
    "    \n",
    "    for u in G:\n",
    "       \n",
    "        Tv=[n for n in G.neighbors(u)] # neighbors of v.\n",
    "        h_index_2=h_index_1_Nodes[(u)]\n",
    "        for n in Tv:\n",
    "            h_index_2+=h_index_1_Nodes[(n)]\n",
    "        h_index_2_Nodes[u]=h_index_2\n",
    "        \n",
    "    for u in G:        \n",
    "        Tv=[n for n in G.neighbors(u)] # neighbors of v.\n",
    "        h_index_3=h_index_2_Nodes[(u)]\n",
    "        for n in Tv:\n",
    "            h_index_3+=h_index_2_Nodes[(n)]\n",
    "        h_index_3_Nodes[u]=h_index_3    \n",
    "        \n",
    "    return h_index_1_Nodes,h_index_2_Nodes,h_index_3_Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1e0cc",
   "metadata": {},
   "source": [
    "# Structural channel sets of node representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc173792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_one_two_hop_Adj_mat_of_node(G,L,node,dic_D1,dic_D2,dic_D3,dic_H1,dic_H2,dic_H3 ):\n",
    "      \n",
    "    one_hop = list(G.adj[node])  \n",
    "    one_hop_weight_a ={}  \n",
    "    for u in one_hop:\n",
    "        one_hop_weight_a[u]=dic_D3[(u)]   \n",
    "    \n",
    "    sorted_list=sorted(one_hop_weight_a.items(),key=lambda x:x[1],reverse=True)   \n",
    "     \n",
    "        \n",
    "    selected_nei = [node]+[i for i,j in sorted_list[:L] ]  \n",
    "    # padding\n",
    "    if len(one_hop) < L:  \n",
    "        selected_nei=selected_nei+[-1 for i in range(L-len(one_hop))]      \n",
    "    # extract ADJ Mat\n",
    "    arr_D1=[]   \n",
    "    arr_D2=[]    \n",
    "    arr_D3=[] \n",
    "    \n",
    "    \n",
    "    arr_H1=[] \n",
    "    arr_H2=[]\n",
    "    arr_H3=[] \n",
    "      \n",
    "    i_index=0 \n",
    "    for i in selected_nei:\n",
    "        col_D1 = []\n",
    "        col_D2 = []                \n",
    "        col_D3 = []\n",
    "       \n",
    "        \n",
    "        col_H1 = [] \n",
    "        col_H2 = []\n",
    "        col_H3 = [] \n",
    "        \n",
    "        j_index=0\n",
    "        for j in selected_nei:\n",
    "            if i_index==j_index:\n",
    "                col_D1.append(dic_D1[(node)])\n",
    "                col_D2.append(dic_D2[(node)])                                              \n",
    "                col_D3.append(dic_D3[(node)])\n",
    "                \n",
    "                \n",
    "                col_H1.append(dic_H1[(node)])\n",
    "                col_H2.append(dic_H2[(node)])\n",
    "                col_H3.append(dic_H3[(node)])\n",
    "                \n",
    "                 \n",
    "                \n",
    "            else:\n",
    "                if G.has_edge(i,j):\n",
    "                    if(i_index==0 and i_index<j_index):\n",
    "                        col_D1.append(dic_D1[(j)])\n",
    "                        col_D2.append(dic_D2[(j)])                      \n",
    "                        col_D3.append(dic_D3[(j)])\n",
    "                        \n",
    "                        col_H1.append(dic_H1[(j)])\n",
    "                        col_H2.append(dic_H2[(j)])\n",
    "                        col_H3.append(dic_H3[(j)])\n",
    "                \n",
    "                    elif(j_index==0 and i_index>j_index):\n",
    "                        col_D1.append(dic_D1[(i)])\n",
    "                        col_D2.append(dic_D2[(i)])                       \n",
    "                        col_D3.append(dic_D3[(i)])\n",
    "                       \n",
    "                        \n",
    "                        col_H1.append(dic_H1[(i)])\n",
    "                        col_H2.append(dic_H2[(i)])\n",
    "                        col_H3.append(dic_H3[(i)])\n",
    "                       \n",
    "                    else:\n",
    "                        col_D1.append(1)\n",
    "                        col_D2.append(1)                     \n",
    "                        col_D3.append(1)                       \n",
    "                        \n",
    "                        col_H1.append(1)\n",
    "                        col_H2.append(1)\n",
    "                        col_H3.append(1)\n",
    "                        \n",
    "                else:\n",
    "                    col_D1.append(0)\n",
    "                    col_D2.append(0)\n",
    "                    col_D3.append(0)                   \n",
    "                    \n",
    "                    col_H1.append(0)\n",
    "                    col_H2.append(0)\n",
    "                    col_H3.append(0)\n",
    "                    \n",
    "                    \n",
    "            j_index+=1\n",
    "        i_index+=1\n",
    "        arr_D1.append(col_D1) \n",
    "        arr_D2.append(col_D2)         \n",
    "        arr_D3.append(col_D3)        \n",
    "        \n",
    "        arr_H1.append(col_H1)\n",
    "        arr_H2.append(col_H2)\n",
    "        arr_H3.append(col_H3)\n",
    "        \n",
    "    return np.array(arr_D1),np.array(arr_D2),np.array(arr_D3),np.array(arr_H1),np.array(arr_H2),np.array(arr_H3)\n",
    "\n",
    "\n",
    "\n",
    "def metrics_one__hop_Adj_mat_of_all_nodes(G,L  ):  \n",
    "    dic_local_embedding={}\n",
    "    dic_semi_embedding={}\n",
    "    \n",
    "    dic_D1=dic_D_1_weights_all_nodes(G)\n",
    "    dic_D2=dic_D_2_weights_all_nodes(G,dic_D1)    \n",
    "    dic_D3=dic_D_2_weights_all_nodes(G,dic_D2)  \n",
    "    \n",
    "    \n",
    "    dic_H1,dic_H2,dic_H3=H_index_weights_of_All_nodes(G)\n",
    "        \n",
    "    for node in G:\n",
    "        hop_1_arr_D1,hop_1_arr_D2,hop_1_arr_D3,hop_1_arr_H1,hop_1_arr_H2,hop_1_arr_H3=metrics_one_two_hop_Adj_mat_of_node(G,L,node,dic_D1,dic_D2, dic_D3,dic_H1,dic_H2,dic_H3 )\n",
    " \n",
    "        dic_local_embedding[node]=hop_1_arr_D1,hop_1_arr_D2, hop_1_arr_D3    \n",
    "        dic_semi_embedding[node]= hop_1_arr_H1,hop_1_arr_H2,hop_1_arr_H3 \n",
    "         \n",
    "    return dic_local_embedding,dic_semi_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0920e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LCNN_Model(path_Train_Data,path_SIR_Train_Data,L,Kernel_size,MaxPooling,  dense,learning_rate,epochN):\n",
    "    \n",
    "    in_channel_L=3\n",
    "    in_channel_S=3 \n",
    "\n",
    "    data_G = loadData(path_Train_Data)\n",
    "    data_G_sir = pd.read_csv(path_SIR_Train_Data)    \n",
    "     \n",
    " \n",
    "    data_G_label = dict(zip(np.array(data_G_sir['Node'],dtype=str),data_G_sir['SIR']))    \n",
    "    \n",
    "    dic_local_embedding,dic_semi_embedding=metrics_one__hop_Adj_mat_of_all_nodes(data_G,L  )\n",
    "     \n",
    "    x1_train = []\n",
    "    x2_train = []\n",
    "      \n",
    "    y_train = []\n",
    "    \n",
    "    for node in data_G:\n",
    "        x1_train.append(dic_local_embedding[(node)])\n",
    "        x2_train.append(dic_semi_embedding[(node)])\n",
    "        \n",
    "        y_train.append(data_G_label[(node)])\n",
    "      \n",
    "    x1_train=np.array(x1_train)\n",
    "    x2_train=np.array(x2_train)\n",
    "         \n",
    "    x1_train=x1_train.reshape(-1, L+1, L+1,  in_channel_L)\n",
    "    x2_train=x2_train.reshape(-1, L+1, L+1, in_channel_S)\n",
    "         \n",
    "    print(x1_train.shape)\n",
    "    print(x2_train.shape)\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    input_shape_L = (L+1, L+1, in_channel_L)\n",
    "    input_shape_S = (L+1, L+1, in_channel_S)\n",
    "        \n",
    "    def create_convolution_layers(input_model,input_shape):\n",
    "        \n",
    "        model = Conv2D(filters=18,kernel_size= Kernel_size,strides=1, padding='same', input_shape=input_shape)(input_model)       \n",
    "        model = keras.layers.BatchNormalization()(model)\n",
    "        model = LeakyReLU(alpha=0.1)(model)\n",
    "        model = MaxPooling2D((MaxPooling, MaxPooling),padding='same')(model)\n",
    "               \n",
    "        \n",
    "        model = Conv2D(filters=48,kernel_size= Kernel_size,strides=1, padding='same')(model)\n",
    "        model = keras.layers.BatchNormalization()(model)\n",
    "        model = LeakyReLU(alpha=0.1)(model)\n",
    "        model = MaxPooling2D((MaxPooling, MaxPooling),padding='same')(model)\n",
    "         \n",
    "        \n",
    "        return model\n",
    "      \n",
    "    \n",
    "    local_input = Input(shape=input_shape_L)\n",
    "    # local_input.shape\n",
    "    conv_1 = create_convolution_layers(local_input,input_shape_L)\n",
    "    conv_1 = keras.layers.TimeDistributed(Flatten())(conv_1) \n",
    "    conv_1=keras.layers.GlobalAveragePooling1D()(conv_1) \n",
    "        \n",
    "    semi_input = Input(shape=input_shape_S)\n",
    "    conv_2 = create_convolution_layers(semi_input,input_shape_S)\n",
    "    conv_2 = keras.layers.TimeDistributed(Flatten())(conv_2)\n",
    "    conv_2=keras.layers.GlobalAveragePooling1D()(conv_2)\n",
    " \n",
    "     \n",
    "    convAall = concatenate([conv_1,conv_2])\n",
    "    \n",
    "    dense = Dense(dense)(convAall) \n",
    "    dense=LeakyReLU(alpha=0.1)(dense)\n",
    "    dense = Dense(1)(dense)\n",
    "    output=LeakyReLU(alpha=0.1)(dense)\n",
    "\n",
    "     \n",
    "    model = Model(inputs=[local_input, semi_input], outputs=[output])\n",
    "    opt= keras.optimizers.Adam(learning_rate=learning_rate)  \n",
    "    model.compile(loss=\"mse\", optimizer=opt)\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit([x1_train,x2_train],y_train,epochs =epochN,shuffle=True,batch_size=4)\n",
    "    \n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    epochs_range = range(epochN)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.subplot(1, 1, 1)  \n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26afcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def loadData(nameDataset,sep=\",\"):    \n",
    "    df= pd.read_csv(nameDataset, sep=sep,names=['FromNodeId','ToNodeId'])       \n",
    "    \n",
    "    G = nx.from_pandas_edgelist(df, source=\"FromNodeId\", target=\"ToNodeId\")    \n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    \n",
    "    G.remove_nodes_from(['FromNodeId', 'ToNodeId',])\n",
    "    #print(nx.info(G))\n",
    "    return G\n",
    "\n",
    "\n",
    "def get_data_to_model(G,L ):\n",
    "    dic_local_embedding,dic_semi_embedding=metrics_one__hop_Adj_mat_of_all_nodes(G,L )\n",
    "\n",
    "    x1_train = []\n",
    "    x2_train = []\n",
    "   \n",
    "    in_channel_L=3\n",
    "    in_channel_S=3\n",
    "    \n",
    "    for node in G:\n",
    "        x1_train.append(dic_local_embedding[(node)])\n",
    "        x2_train.append(dic_semi_embedding[(node)])\n",
    "        \n",
    "    x1_train=np.array(x1_train)\n",
    "    x2_train=np.array(x2_train)\n",
    "    \n",
    "    x1_train=x1_train.reshape(-1, L+1, L+1, in_channel_L)\n",
    "    x2_train=x2_train.reshape(-1, L+1, L+1, in_channel_S)\n",
    "    \n",
    "    # print(x1_train.shape)\n",
    "    # print(x2_train.shape)  \n",
    "    return x1_train,x2_train    \n",
    "\n",
    "def get_sir_list(pathDataset,nameDataset,sir_rang_list):\n",
    "    sir_list=[]   \n",
    "    for a_tau in sir_rang_list:\n",
    "        sir = pd.read_csv(pathDataset+nameDataset+'/'+nameDataset+'_a['+str(round(a_tau,1))+']_.csv')\n",
    "       \n",
    "        sir_list.append(dict(zip(np.array(sir['Node'],dtype=str),sir['SIR'])))\n",
    "    return sir_list\n",
    "\n",
    "def nodesRank(rank):\n",
    "    SR = sorted(rank)\n",
    "    re = []\n",
    "    for i in SR:\n",
    "        re.append(rank.index(i))\n",
    "    return re\n",
    "\n",
    "def get_algo_list(pathDataset,dataName,algoName):\n",
    "    algo_list=[]\n",
    "    df = pd.read_csv(pathDataset)     \n",
    "    df=df[df['Dataset']==dataName]\n",
    "    df=df[df['Algo']==algoName]    \n",
    "    algo_list=literal_eval(df['Seed'].iloc[0])\n",
    "    algo_list=algo_list         \n",
    "    return algo_list\n",
    "               \n",
    "def compare_tau(sir_list,alg_list):   \n",
    "    alg_tau_list=[]   \n",
    "    for sir in sir_list:        \n",
    "        sir_sort = [i for i,j in sorted(sir.items(),key=lambda x:x[1],reverse=True)]     \n",
    "        tau3,_ = stats.kendalltau(nodesRank(alg_list),nodesRank(sir_sort))            \n",
    "        alg_tau_list.append(tau3)        \n",
    "    return alg_tau_list    \n",
    "\n",
    "\n",
    "def rank_dataset_using_LCNN(model,model_name,input_Datasets_to_pred,path_input_Datasets,path_SIR_input_Datasets,\n",
    "                            sir_rang_list,path_saved_ranked_node,L,  \n",
    "                             name_Train_Data,sir_a_value_Train_Data ,Kernel_size,MaxPooling,  Dense,learning_rate):\n",
    "    df_seed_LCNN = pd.DataFrame( columns=['Dataset','Algo','Seed','time'])\n",
    "    df_tau_result = pd.DataFrame( columns=['Dataset', 'sir_a_value_Train_Data',  \n",
    "                                           'Algo','Tau','Dense','MaxPooling','Kernel_size','learning_rate'])\n",
    "\n",
    "    for dataName in input_Datasets_to_pred:\n",
    "        start_time = time.time()   \n",
    "        G = loadData(path_input_Datasets+dataName+'.csv')\n",
    "        x1_train,x2_train=get_data_to_model(G,L  )\n",
    "        data_predictions = model.predict([x1_train,x2_train])\n",
    "        nodes = list(G.nodes())\n",
    "        my_pred = [i for i,j in sorted(dict(zip(nodes,data_predictions)).items(),key=lambda x:x[1],reverse=True)] \n",
    "        timelapse=(time.time() - start_time)   \n",
    "        #df2 = {'Dataset': dataName, 'Algo': model_name, 'Seed': my_pred,'time':timelapse}\n",
    "        #df_seed_LCNN=df_seed_LCNN.append(df2, ignore_index = True)\n",
    "        \n",
    "        print('-------------------------------------------------------------')\n",
    "        print('done', model_name,' in  ', dataName)\n",
    "        print('-------------------------------------------------------------')\n",
    "        G_SIR = get_sir_list(path_SIR_input_Datasets,dataName,sir_rang_list)        \n",
    "        tau=compare_tau(G_SIR,my_pred)\n",
    "        print('tau=',tau)\n",
    "        df3 = {'Dataset': dataName, \n",
    "               'sir_a_value_Train_Data':sir_a_value_Train_Data,   'Algo': model_name, 'Tau': tau,\n",
    "              'Dense':Dense,'MaxPooling':MaxPooling,'Kernel_size':Kernel_size,'learning_rate':learning_rate}\n",
    "        df_tau_result=df_tau_result.append(df3, ignore_index = True)\n",
    "        \n",
    "        \n",
    "        #df_seed_LCNN.to_csv(model_name+'__Seed.csv')\n",
    "        df_tau_result.to_csv(path_saved_ranked_node+'/'+model_name+'__Tau.csv')\n",
    "    \n",
    "  \n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d264ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 3s 10ms/step\n",
      "-------------------------------------------------------------\n",
      "done LCNN_Ker_2_Max_2_dense_1024lear_0.0005  in   BA-8k\n",
      "-------------------------------------------------------------\n",
      "tau= [0.8263864858107262, 0.8373622327790973, 0.8389626828353544, 0.8462620327540942, 0.8433811726465807, 0.8461805225653205, 0.848389861232654, 0.8510750718839855, 0.853206150768846, 0.857373859232404]\n",
      "254/254 [==============================] - 3s 10ms/step\n",
      "-------------------------------------------------------------\n",
      "done LCNN_Ker_2_Max_2_dense_1024lear_0.0005  in   Gnutella\n",
      "-------------------------------------------------------------\n",
      "tau= [0.8309151293196805, 0.8478717593897462, 0.8688195859075959, 0.8837907045117369, 0.8962815136371295, 0.9058871454022263, 0.9093900455426236, 0.9088144319388562, 0.9061511632538434, 0.9023494884813628]\n"
     ]
    }
   ],
   "source": [
    "def reset_random_seeds():\n",
    "    os.environ['PYTHONHASHSEED']=str(2)\n",
    "    tf.random.set_seed(2)\n",
    "    np.random.seed(2)\n",
    "    random.seed(2)\n",
    "reset_random_seeds()\n",
    "\n",
    "L=40\n",
    "epochN=200\n",
    "dense=1024\n",
    "learning_ra=0.0005  \n",
    "MaxPooling=2 \n",
    "Kernel_size=2 \n",
    "\n",
    "name_Train_Data='BA-1k.csv' \n",
    "sir_a_value_Train_Data='1.5' \n",
    "path_saved_ranked_node='Results'\n",
    "sir_rang_list = np.arange(1.0,2.0,0.1)\n",
    "\n",
    "\n",
    "input_Datasets_to_pred=['BA-8k','Gnutella' ]\n",
    "sir_rang_list = np.arange(1.0,2.0,0.1)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "path_Train_Data='Data/'+name_Train_Data\n",
    "path_SIR_Train_Data='SIR/'+name_Train_Data[:-4]+'/'+name_Train_Data[:-4]+'_a['+sir_a_value_Train_Data+']_.csv'\n",
    "\n",
    "         \n",
    "model_name='LCNN'+'_Ker_'+str(Kernel_size)+'_Max_'+str(MaxPooling)+'_dense_'+str(dense)+'lear_'+str(learning_ra) \n",
    "PATH_saved_model = \"Models/\"+model_name+\".h5\"\n",
    "\n",
    "#model,_=create_LCNN_Model(path_Train_Data,path_SIR_Train_Data,L,Kernel_size,MaxPooling, dense,learning_ra,epochN  )\n",
    "#model.save(PATH_saved_model) \n",
    "\n",
    "model=models.load_model(PATH_saved_model, compile=False)\n",
    "path_input_Datasets='Data/'\n",
    "path_SIR_input_Datasets='SIR/'\n",
    "\n",
    "rank_dataset_using_LCNN(model,model_name,input_Datasets_to_pred,path_input_Datasets,path_SIR_input_Datasets,\n",
    "                        sir_rang_list,path_saved_ranked_node,L,\n",
    "                        name_Train_Data,sir_a_value_Train_Data ,Kernel_size,MaxPooling,  dense,learning_ra )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b08ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
